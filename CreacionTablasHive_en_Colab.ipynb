{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ohmono/clustering-spark/blob/main/CreacionTablasHive_en_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTSupE3dten4"
      },
      "source": [
        "\n",
        "<p><img alt=\"udeA logo\" height=\"120px\" src=\"https://github.com/freddyduitama/images/blob/master/logo.png?raw=true\" align=\"left\" hspace=\"10px\" vspace=\"0px\" style=\"width:107px;height:152px;\"></p>\n",
        "\n",
        "# <center> <font color='0B5345'>Creación de tablas y particiones con HIVE.</font> </center>\n",
        "##### <center><font face=\"Verdana\"><a  href=\"https://colab.research.google.com/drive/13L6BpqgXtFdd9B-bT3Sr9ZksQ5U1dRlO#scrollTo=QtHm4ClUKJTI\">El diseño físico y el Big Data.</a><a>&nbsp;&nbsp; | </a><a  href=\"https://colab.research.google.com/drive/1Ddb-oi4v_FaJPOi2LrdAENitLC8GWpu5#scrollTo=-O6sHpqcFnVCb\" >TOC</a><a>&nbsp;  |</a> <a>&nbsp;&nbsp;</a> <a   href=\"https://colab.research.google.com/drive/1ICKacknjauqwsLGirbURae5IUdHX759d\">Map Reduce.</a></font><center><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njYmfaVkt9ds"
      },
      "source": [
        "## 1. Introducción\n",
        "\n",
        "<font face=\"Verdana\" size=\"2\">\n",
        "<p align=\"justify\">\n",
        "<em>Apache Hive™</em> es una bodega de datos construida sobre <em>Apache Hadoop</em> para proporcionar consultas y análisis de datos[3].\n",
        "El software de almacén de datos <em>Apache Hive™</em> facilita la lectura, escritura y gestión de grandes conjuntos de datos que residen en el almacenamiento distribuido utilizando SQL. Hive ofrece una interfaz similar a SQL para consultar datos almacenados en varias bases de datos y sistemas de archivos que se integran con Hadoop.  Aunque inicialmente fue desarrollado por <em>Facebook™</em>, Apache Hive es utilizado y desarrollado por otras empresas como Netflix y la Autoridad Reguladora de la Industria Financiera (FINRA). <em>Amazon™</em> mantiene un fork de software de Apache Hive incluido en <em>Amazon Elastic MapReduce</em> en Amazon Web Services.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVSCHzjLlZd9"
      },
      "source": [
        "## 2. Ejemplos de SQL con HIVE\n",
        "\n",
        "### 2.1 Configurando el ambiente de trabajo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1l98HKPx5pl"
      },
      "source": [
        "<font face=\"Verdana\" size=\"2\">\n",
        "<p align=\"justify\">\n",
        "<b>Paso 1:</b> Verifique la librería de SPARK más reciente <a href=\"http://apache.osuosl.org/spark/\">Acá</a>. Si esta es diferente a la especificada abajo, debe cambiar la versión de SPARK y de HADOOP en las dos siguientes celdas\n",
        "\n",
        "- Las instrucciones:  !wget -q......tar...\n",
        "- También debe actualizar las variables de ambiente de JAVA.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alYd_PMvWeMQ"
      },
      "source": [
        "# Estas intrucciones instalan el ambiente de spark y de hadoop (el DFS que vamos a usar)..solo se corre una vez\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-3.2.2/spark-3.2.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvz9ToTpTgEM"
      },
      "source": [
        "#En esta celda se configuran las variables de ambiente necesarias para correr los programas\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaGsxDaOzWSl"
      },
      "source": [
        "<font face=\"Verdana\" size=\"2\">\n",
        "<p align=\"justify\">\n",
        "<b>Paso 2:</b>  \n",
        "A continuación se importan las librerías de SPARK y de HIVE.\n",
        "\n",
        "- SPARK es el framework que implementa una versión extendida de Map-Reduce, el paradigma de programación paralela y distribuida que usaremos.\n",
        "- HIVE  es el la bodega de datos basada en SQL de almacenamiento distribuido de la información que corre sobre HADOOP.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz22FmAIYIvO"
      },
      "source": [
        "#importa pyspark package\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import HiveContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFLSDmPvNC88"
      },
      "source": [
        "<font face=\"Verdana\" size=\"2\">\n",
        "<p align=\"justify\">\n",
        "<b>Paso 3:</b>\n",
        "Acá se configura la sesión de trabajo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjngtzOcT_QR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0bfde67-aada-48ef-88a6-e6ca962004f2"
      },
      "source": [
        "# define  la sesion SPARK.\n",
        "conf = SparkConf().setAppName(\"ejemplo\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "sqlContextHive = HiveContext(sc)          # deprecated\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.2-bin-hadoop3.2/python/pyspark/sql/context.py:604: FutureWarning: HiveContext is deprecated in Spark 2.0.0. Please use SparkSession.builder.enableHiveSupport().getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# crea la sesión SPARK.\n",
        "spark = SparkSession.builder\n",
        "spark = spark.enableHiveSupport()\n",
        "spark = spark.config(conf)"
      ],
      "metadata": {
        "id": "-YBfrYsoQ_3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF-Z-Fq1OnFd"
      },
      "source": [
        "## 2.2 Creando la Base de Datos en HIVE.\n",
        "<font face=\"Verdana\" size=\"2\">\n",
        "<p align=\"justify\">\n",
        "<b>Paso 4:</b>\n",
        "Acá se crea la base de datos de trabajo llamada <em>test</em>. Por defecto HIVE tiene una base de datos llamada <em>default</em>.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmKEv5TY3shf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "876d745b-65ae-470d-cbec-5a3ba3174791"
      },
      "source": [
        "#Acá se crea la base de datos de prueba. Solo se crea una vez la base de datos.\n",
        "sqlContextHive.sql('DROP DATABASE IF EXISTS test')\n",
        "sqlContextHive.sql('CREATE DATABASE test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB2jchMv3gpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56caba1-eee6-4548-85cf-2844b9848126"
      },
      "source": [
        "sqlContextHive.sql('show databases').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|namespace|\n",
            "+---------+\n",
            "|  default|\n",
            "|     test|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_e_MaejuLPs"
      },
      "source": [
        "<font face=\"Verdana\" size=\"2\">\n",
        "<p align=\"justify\">\n",
        "<b>Paso 5:</b>\n",
        "Para poder usar google drive como su directorio de trabajo, ejecute la siguiente instrucción e ingrese el <em>código de autorización solicitado</em>.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooazKJfAuKQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d43b569-d893-4786-e2a5-2ffeff21dc36"
      },
      "source": [
        "# mount your google driver\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvPAzonyuqgl"
      },
      "source": [
        "<font face=\"Verdana\" size=\"2\">\n",
        "<p align=\"justify\">\n",
        "<b>Paso 6:</b>\n",
        "Descargue los archivo con los los datos que vamos a usar desde <a href=\"https://drive.google.com/drive/u/0/folders/1D235z6ThQzo3JYE0bLIl1vXzxM9Yl_PP\">acá</a>. Súbalo a su google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqmWeVLxupX8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f373bf-2b08-4494-bc5c-250e693cfe8e"
      },
      "source": [
        "#Verifique el path de los archivos de datos a ser cargados posteriormente en la base de datos.\n",
        "# Debe adecuar esta instrucción a la ubicación que le asigne en su drive.\n",
        "!ls -l '/gdrive/My Drive/Colab Notebooks/BdeD/Data/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 15480\n",
            "-rw------- 1 root root 10327345 Jul 13  2021  auth.log\n",
            "-rw------- 1 root root     8910 Jul 13  2021  barrios.csv\n",
            "-rw------- 1 root root     2505 Mar 31  2021 'base de datos (1).rar'\n",
            "-rw------- 1 root root  3602146 Jul 13  2021  clientes.csv\n",
            "-rw------- 1 root root  1899304 Jul 13  2021  dispositivos.csv\n",
            "drwx------ 2 root root     4096 Aug 31 14:16  file01\n",
            "drwx------ 2 root root     4096 Aug 30 03:29  file02\n",
            "-rw------- 1 root root      528 Mar 18  2021  sem2020-2.csv\n",
            "-rw------- 1 root root      416 Mar 18  2021  sem2021-1.csv\n",
            "-rw------- 1 root root      416 Mar 18  2021  sem2021-2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOTFtzeA57C1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f9f7be-ec34-499f-f0ac-bc127e80962c"
      },
      "source": [
        "# Por precaución borramos las tablas a ser creadas\n",
        "sqlContextHive.sql(\"DROP TABLE IF EXISTS test.alumnos_grupo\")\n",
        "sqlContextHive.sql(\"DROP TABLE IF EXISTS test.alumnos_grupo_paticionado\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR7wFPgzv5Jd"
      },
      "source": [
        "## 2.3 Creando una tabla sin especificar las particiones.\n",
        "<font face=\"Verdana\" size=\"2\">\n",
        "<p align=\"justify\">\n",
        "<b>Paso 7:</b>\n",
        "Vamos a proceder a crear una tabla HIVE. No se especifica ningún tipo de partición. Este sentencia es equivalente a la partición <em>round-robin</em>, es decir no conocemos donde queda ubicado cada registro. Hadoop particiona el archivo de la BdeD, pero no hay control como se ubican los registros por partición. Note que puedo indicar los archivos en Hadoop que deseo usar con la tabla.<br>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2_nVdma9sh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c93c77da-afad-4a39-ee44-1149804a8e14"
      },
      "source": [
        "# Borra la tabla si existe y luego la crea de nuevo, indicando el archivos Hadoop DFS a usar.\n",
        "sqlContextHive.sql(\"DROP TABLE IF EXISTS test.alumnos_grupo\")\n",
        "sqlContextHive.sql(\"CREATE TABLE IF NOT EXISTS test.alumnos_grupo( \\\n",
        "        materia             STRING COMMENT 'código de la materia',\\\n",
        "        grupo               INT,\\\n",
        "        alumno              INT        COMMENT 'Cédula del estudiante',\\\n",
        "        nota                DOUBLE,\\\n",
        "        semestre            STRING) \\\n",
        "        ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \\\n",
        "        LOCATION '/gdrive/My Drive/Colab Notebooks/BdeD/Data/file01';\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1iILMnfyyv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90371e03-932d-4305-d38d-68a3dbf7c6c0"
      },
      "source": [
        "# muestra la descripción de la tabla.\n",
        "sqlContextHive.sql(\"DESCRIBE test.alumnos_grupo\" ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+--------------------+\n",
            "|col_name|data_type|             comment|\n",
            "+--------+---------+--------------------+\n",
            "| materia|   string|código de la materia|\n",
            "|   grupo|      int|                null|\n",
            "|  alumno|      int|Cédula del estudi...|\n",
            "|    nota|   double|                null|\n",
            "|semestre|   string|                null|\n",
            "+--------+---------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtk6hxPE2GJV"
      },
      "source": [
        "<font face=\"Verdana\" size=\"2\">\n",
        "<p align=\"justify\">\n",
        "<b>Paso 8:</b>\n",
        "Vamos a proceder a cargar los datos desde un archivo <em>csv</em> ubicado en el path especificado. Note que hay archivos que no indican el semetre al que corresponde la información.<br>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwKQzyAs80Cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5500d324-6241-4ce3-acda-833079e85783"
      },
      "source": [
        "# cargamos los datos desde el archivo indicado\n",
        "sqlContextHive.sql(\"LOAD DATA LOCAL INPATH '/gdrive/My Drive/Colab Notebooks/BdeD/Data/sem2020-2.csv' INTO TABLE test.alumnos_grupo\" )\n",
        "sqlContextHive.sql(\"LOAD DATA LOCAL INPATH '/gdrive/My Drive/Colab Notebooks/BdeD/Data/sem2021-1.csv' INTO TABLE test.alumnos_grupo\")\n",
        "sqlContextHive.sql(\"LOAD DATA LOCAL INPATH '/gdrive/My Drive/Colab Notebooks/BdeD/Data/sem2021-2.csv' INTO TABLE test.alumnos_grupo\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VGoZnqJ1vLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b96bbf-34f1-49e5-a409-692e4f00db48"
      },
      "source": [
        "# muestra los datos cargados en la tabla. Los atributos en  null no fueron especificados en los archivos de carga, es decir no tienen semestre.\n",
        "sqlContextHive.sql(\"SELECT * FROM test.alumnos_grupo\").show(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+----------+----+--------+\n",
            "|materia|grupo|    alumno|nota|semestre|\n",
            "+-------+-----+----------+----+--------+\n",
            "| IIN101|    1|1018999677| 3.5|  2020-2|\n",
            "| IIN101|    1|1019000786| 4.8|  2020-2|\n",
            "| IIN101|    1|1078499677| 3.5|  2020-2|\n",
            "| IIN101|    1|1019000786| 3.9|  2020-2|\n",
            "| IEO121|    1|1018999677| 3.8|  2020-2|\n",
            "| IEO121|    1|1089430786| 3.8|  2020-2|\n",
            "| IEO121|    1|1018499677| 5.0|  2020-2|\n",
            "| IEO121|    1|1039030786| 2.8|  2020-2|\n",
            "| IEO101|    1|1068939677| 4.5|  2020-2|\n",
            "| IEO101|    1|1029030786| 3.8|  2020-2|\n",
            "| IEO101|    1|1978039677| 4.5|  2020-2|\n",
            "| ISI101|    1|1039530786| 3.8|  2020-2|\n",
            "| ISI101|    1|1028949677| 2.5|  2020-2|\n",
            "| ISI101|    1|1059035786| 4.8|  2020-2|\n",
            "| ISI101|    1|1038059677| 4.5|  2020-2|\n",
            "| ISI101|    1|1079564786| 3.7|  2020-2|\n",
            "| ISI101|    1|1018999677| 3.5|    null|\n",
            "| ISI101|    1|1019000786| 3.8|    null|\n",
            "| ISI101|    1|1078499677| 5.5|    null|\n",
            "| ISI101|    1|1019000786| 2.9|    null|\n",
            "| ISI101|    2|1018999677| 3.8|    null|\n",
            "| ISI101|    2|1089430786| 3.8|    null|\n",
            "| ISI101|    2|1018499677| 5.0|    null|\n",
            "| ISI101|    2|1039030786| 2.8|    null|\n",
            "| ISI101|    3|1068939677| 4.5|    null|\n",
            "| ISI101|    3|1029030786| 3.8|    null|\n",
            "| ISI101|    4|1978039677| 4.5|    null|\n",
            "| ISI101|    4|1039530786| 3.8|    null|\n",
            "| ISI101|    4|1028949677| 2.5|    null|\n",
            "| ISI101|    4|1059035786| 4.8|    null|\n",
            "| ISI101|    4|1038059677| 4.5|    null|\n",
            "| ISI101|    4|1079564786| 3.7|    null|\n",
            "| IEO101|    1|1018999677| 3.5|    null|\n",
            "| IEO101|    1|1019000786| 3.8|    null|\n",
            "| IEO101|    1|1078499677| 5.5|    null|\n",
            "| IEO101|    1|1019000786| 2.9|    null|\n",
            "| IEO101|    2|1018999677| 3.8|    null|\n",
            "| IEO101|    2|1089430786| 3.8|    null|\n",
            "| IEO101|    2|1018499677| 5.0|    null|\n",
            "| IEO101|    2|1039030786| 2.8|    null|\n",
            "| IEO101|    3|1068939677| 4.5|    null|\n",
            "| IEO101|    3|1029030786| 3.8|    null|\n",
            "| IEO101|    4|1978039677| 4.5|    null|\n",
            "| ISI101|    1|1039530786| 3.8|    null|\n",
            "| ISI101|    1|1028949677| 2.5|    null|\n",
            "| ISI101|    1|1059035786| 4.8|    null|\n",
            "| ISI101|    1|1038059677| 4.5|    null|\n",
            "| ISI101|    1|1079564786| 3.7|    null|\n",
            "| IIN101|    1|1018999677| 3.5|  2020-2|\n",
            "| IIN101|    1|1019000786| 4.8|  2020-2|\n",
            "+-------+-----+----------+----+--------+\n",
            "only showing top 50 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reBvGIUs2A_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4bf95ae-c76a-4520-b576-ea2739f27666"
      },
      "source": [
        "# muestra la ubicación en disco de los archivos donde se almacena la tabla HIVE.\n",
        "# Note que los nombres de los archivos no están asociados a particiones lógicas, muchos registros no tienen semestre asociado.\n",
        "!ls -l '/gdrive/My Drive/Colab Notebooks/BdeD/Data/file01'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "-rw------- 1 root root 528 Aug 31 14:12 sem2020-2_copy_1.csv\n",
            "-rwx------ 1 root root 528 Aug 31 14:16 sem2020-2_copy_2.csv\n",
            "-rwx------ 1 root root 528 Aug 31 14:20 sem2020-2_copy_3.csv\n",
            "-rw------- 1 root root 528 Aug 30 13:26 sem2020-2.csv\n",
            "-rw------- 1 root root 416 Aug 31 14:12 sem2021-1_copy_1.csv\n",
            "-rwx------ 1 root root 416 Aug 31 14:16 sem2021-1_copy_2.csv\n",
            "-rwx------ 1 root root 416 Aug 31 14:20 sem2021-1_copy_3.csv\n",
            "-rw------- 1 root root 416 Aug 30 13:26 sem2021-1.csv\n",
            "-rw------- 1 root root 416 Aug 31 14:12 sem2021-2_copy_1.csv\n",
            "-rwx------ 1 root root 416 Aug 31 14:16 sem2021-2_copy_2.csv\n",
            "-rwx------ 1 root root 416 Aug 31 14:20 sem2021-2_copy_3.csv\n",
            "-rw------- 1 root root 416 Aug 30 13:26 sem2021-2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a4QPzVwUsrG"
      },
      "source": [
        "## 2.4. Creando particiones en HIVE\n",
        "<font face=\"Verdana\" size=\"2\">\n",
        "<p align=\"justify\">\n",
        "<b>Paso 9:</b> Vamos a proceder a crear una tabla HIVE especificando las particiones deseadas. Suponga que tiene una bodega de datos como la de la figura 1.\n",
        "\n",
        "<center><img src=\"https://github.com/freddyduitama/figuras/blob/main/bodega.png?raw=true\"  height=\"300\" width=\"600\"></center>\n",
        "<caption><center><font color='0B5345'> <u> <b>Figura 1: Bodega de datos con la historia académica.</b><br> </u> .</font></center></caption>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s88UFT4n4I1"
      },
      "source": [
        "<font face=\"Verdana\" size=\"2\">\n",
        "<p align=\"justify\">\n",
        "Vamos a proceder a crear una tabla HIVE para almacenar los hechos de la bodega especificando las particiones deseadas. La operación que vamos a realizar en el ejemplo consta de dos pasos\n",
        "\n",
        "- Nuestro propósito es crear una partición por cada semestre académico; es decir, en cada partición se almacenará la información de los alumnos matriculados en los grupos abiertos por cada semestre académico cursado. Se define una partición por lista de valores. Es decir, indicamos el valor o los valores asociados a una columna que serán almacenados en cada partición.\n",
        "- Dentro de cada partición previa queremos almacenar los alumnos de un mismo grupo en una misma cubeta, usamos entonces una partición hash. Dentro de cada partición creada previamente, se crean cinco cubetas para almacenar allí los registros. HIVE  usa una function hash que los ubica según la materia y el grupo de cada estudiante matriculado.\n",
        "\n",
        "<center><img src=\"https://github.com/freddyduitama/figuras/blob/main/particion-hive.png?raw=true\"  height=\"150\" width=\"600\"></center>\n",
        "<caption><center><font color='0B5345'> <u> <b>Figura 2: ¨Particiones requeridas sobre la tabla Alumnos_grupo.</b><br> </u> .</font></center></caption>\n",
        "\n",
        "- La sintaxis  para de la creación de las particiones la encuentra en la siguiente celda. Para consultar todas las variantes que ofrece HIVE para crear particiones puede leer <a href=\"https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-create-table-hiveformat.html\">acá </a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OreAZiNN7ZxF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd5cac7e-3dd1-4672-d595-4921ecdeb337"
      },
      "source": [
        "sqlContextHive.sql(\"DROP TABLE IF EXISTS test.alumnos_grupo_particionado\")\n",
        "sqlContextHive.sql(\"CREATE TABLE IF NOT EXISTS test.alumnos_grupo_particionado( \\\n",
        "        materia             STRING COMMENT 'código de la materia',\\\n",
        "        grupo               INT,\\\n",
        "        alumno              INT        COMMENT 'Cédula del estudiante',\\\n",
        "        nota                DOUBLE)  \\\n",
        "        PARTITIONED BY (semestre  STRING) \\\n",
        "        CLUSTERED BY (materia,grupo) \\\n",
        "        INTO 5 BUCKETS \\\n",
        "        ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \\\n",
        "        LOCATION '/gdrive/My Drive/Colab Notebooks/BdeD/Data/file02';\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WVM0ZfXXiRU"
      },
      "source": [
        "<font face=\"Verdana\" size=\"2\">\n",
        "<p align=\"justify\">\n",
        "<b>Paso 9:</b> Vamos a proceder a cargar los datos indicando la partición destino de acda archivo cargado.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-C59r3hXhY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee8ab5b9-a911-4f19-97fb-ae07d1a43185"
      },
      "source": [
        "# cargamos los datos desde el archivo indicado\n",
        "sqlContextHive.sql(\"LOAD DATA LOCAL INPATH '/gdrive/My Drive/Colab Notebooks/BdeD/Data/sem2020-2.csv' INTO TABLE test.alumnos_grupo_particionado PARTITION(semestre='2020-2')\")\n",
        "sqlContextHive.sql(\"LOAD DATA LOCAL INPATH '/gdrive/My Drive/Colab Notebooks/BdeD/Data/sem2021-1.csv' INTO TABLE test.alumnos_grupo_particionado PARTITION(semestre='2021-1')\")\n",
        "sqlContextHive.sql(\"LOAD DATA LOCAL INPATH '/gdrive/My Drive/Colab Notebooks/BdeD/Data/sem2021-2.csv' INTO TABLE test.alumnos_grupo_particionado PARTITION(semestre='2021-2')\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK_t_GCU9Q5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b595426-5492-4c57-81d9-40afbaea5775"
      },
      "source": [
        "sqlContextHive.sql(\"SELECT * FROM test.alumnos_grupo_particionado\").show(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+----------+----+--------+\n",
            "|materia|grupo|    alumno|nota|semestre|\n",
            "+-------+-----+----------+----+--------+\n",
            "| IIN101|    1|1018999677| 3.5|  2020-2|\n",
            "| IIN101|    1|1019000786| 4.8|  2020-2|\n",
            "| IIN101|    1|1078499677| 3.5|  2020-2|\n",
            "| IIN101|    1|1019000786| 3.9|  2020-2|\n",
            "| IEO121|    1|1018999677| 3.8|  2020-2|\n",
            "| IEO121|    1|1089430786| 3.8|  2020-2|\n",
            "| IEO121|    1|1018499677| 5.0|  2020-2|\n",
            "| IEO121|    1|1039030786| 2.8|  2020-2|\n",
            "| IEO101|    1|1068939677| 4.5|  2020-2|\n",
            "| IEO101|    1|1029030786| 3.8|  2020-2|\n",
            "| IEO101|    1|1978039677| 4.5|  2020-2|\n",
            "| ISI101|    1|1039530786| 3.8|  2020-2|\n",
            "| ISI101|    1|1028949677| 2.5|  2020-2|\n",
            "| ISI101|    1|1059035786| 4.8|  2020-2|\n",
            "| ISI101|    1|1038059677| 4.5|  2020-2|\n",
            "| ISI101|    1|1079564786| 3.7|  2020-2|\n",
            "| ISI101|    1|1018999677| 3.5|  2021-1|\n",
            "| ISI101|    1|1019000786| 3.8|  2021-1|\n",
            "| ISI101|    1|1078499677| 5.5|  2021-1|\n",
            "| ISI101|    1|1019000786| 2.9|  2021-1|\n",
            "| ISI101|    2|1018999677| 3.8|  2021-1|\n",
            "| ISI101|    2|1089430786| 3.8|  2021-1|\n",
            "| ISI101|    2|1018499677| 5.0|  2021-1|\n",
            "| ISI101|    2|1039030786| 2.8|  2021-1|\n",
            "| ISI101|    3|1068939677| 4.5|  2021-1|\n",
            "| ISI101|    3|1029030786| 3.8|  2021-1|\n",
            "| ISI101|    4|1978039677| 4.5|  2021-1|\n",
            "| ISI101|    4|1039530786| 3.8|  2021-1|\n",
            "| ISI101|    4|1028949677| 2.5|  2021-1|\n",
            "| ISI101|    4|1059035786| 4.8|  2021-1|\n",
            "| ISI101|    4|1038059677| 4.5|  2021-1|\n",
            "| ISI101|    4|1079564786| 3.7|  2021-1|\n",
            "| IEO101|    1|1018999677| 3.5|  2021-2|\n",
            "| IEO101|    1|1019000786| 3.8|  2021-2|\n",
            "| IEO101|    1|1078499677| 5.5|  2021-2|\n",
            "| IEO101|    1|1019000786| 2.9|  2021-2|\n",
            "| IEO101|    2|1018999677| 3.8|  2021-2|\n",
            "| IEO101|    2|1089430786| 3.8|  2021-2|\n",
            "| IEO101|    2|1018499677| 5.0|  2021-2|\n",
            "| IEO101|    2|1039030786| 2.8|  2021-2|\n",
            "| IEO101|    3|1068939677| 4.5|  2021-2|\n",
            "| IEO101|    3|1029030786| 3.8|  2021-2|\n",
            "| IEO101|    4|1978039677| 4.5|  2021-2|\n",
            "| ISI101|    1|1039530786| 3.8|  2021-2|\n",
            "| ISI101|    1|1028949677| 2.5|  2021-2|\n",
            "| ISI101|    1|1059035786| 4.8|  2021-2|\n",
            "| ISI101|    1|1038059677| 4.5|  2021-2|\n",
            "| ISI101|    1|1079564786| 3.7|  2021-2|\n",
            "+-------+-----+----------+----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5bc9UX8NmEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049dfc6b-6219-4898-ce7b-2dff7a9e7063"
      },
      "source": [
        "sqlContextHive.sql(\"Show partitions test.alumnos_grupo_particionado\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|partition      |\n",
            "+---------------+\n",
            "|semestre=2020-2|\n",
            "|semestre=2021-1|\n",
            "|semestre=2021-2|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCe4hyeN5FNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a18f88be-50d0-47e4-b1c8-096ff710d834"
      },
      "source": [
        "# muestra la ubicación en disco de los archivos donde se almacena la tabla HIVE\n",
        "!ls -l '/gdrive/My Drive/Colab Notebooks/BdeD/Data/file02'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12\n",
            "drwx------ 2 root root 4096 Aug 31 14:20 'semestre=2020-2'\n",
            "drwx------ 2 root root 4096 Aug 31 14:20 'semestre=2021-1'\n",
            "drwx------ 2 root root 4096 Aug 31 14:20 'semestre=2021-2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60cOULDO5wlb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "572d5683-9d8a-4cff-b41b-b79bddcf0d08"
      },
      "source": [
        "# muestra la descripción de la tabla.\n",
        "sqlContextHive.sql(\"DESCRIBE FORMATTED test.alumnos_grupo_particionado\" ).show(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|            col_name|           data_type|             comment|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|             materia|              string|código de la materia|\n",
            "|               grupo|                 int|                null|\n",
            "|              alumno|                 int|Cédula del estudi...|\n",
            "|                nota|              double|                null|\n",
            "|            semestre|              string|                null|\n",
            "|# Partition Infor...|                    |                    |\n",
            "|          # col_name|           data_type|             comment|\n",
            "|            semestre|              string|                null|\n",
            "|                    |                    |                    |\n",
            "|# Detailed Table ...|                    |                    |\n",
            "|            Database|                test|                    |\n",
            "|               Table|alumnos_grupo_par...|                    |\n",
            "|               Owner|                root|                    |\n",
            "|        Created Time|Wed Aug 31 14:20:...|                    |\n",
            "|         Last Access|             UNKNOWN|                    |\n",
            "|          Created By|         Spark 3.2.2|                    |\n",
            "|                Type|            EXTERNAL|                    |\n",
            "|            Provider|                hive|                    |\n",
            "|         Num Buckets|                   5|                    |\n",
            "|      Bucket Columns|[`materia`, `grupo`]|                    |\n",
            "|        Sort Columns|                  []|                    |\n",
            "|    Table Properties|[transient_lastDd...|                    |\n",
            "|            Location|file:/gdrive/My%2...|                    |\n",
            "|       Serde Library|org.apache.hadoop...|                    |\n",
            "|         InputFormat|org.apache.hadoop...|                    |\n",
            "|        OutputFormat|org.apache.hadoop...|                    |\n",
            "|  Storage Properties|[serialization.fo...|                    |\n",
            "|  Partition Provider|             Catalog|                    |\n",
            "+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necesario para borrar BdeD test\n",
        "sqlContextHive.sql(\"DROP TABLE IF EXISTS test.alumnos_grupo\")\n",
        "sqlContextHive.sql(\"DROP TABLE IF EXISTS test.alumnos_grupo\")\n",
        "sqlContextHive.sql(\"DROP TABLE IF EXISTS test.alumnos_grupo_particionado\")"
      ],
      "metadata": {
        "id": "k_Xz0vm60Stf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88727d7a-af14-46d2-8b0c-5a5f22161756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2WXC7JAuaFb"
      },
      "source": [
        "# Bibliografía\n",
        "<p align=\"justify\">\n",
        "[1]&nbsp;&nbsp;&nbsp;&nbsp;Apache Software Foundation. <cite>Apache Hive 2.0. </cite><a href=\"https://hive.apache.org/\"> Click</a> </p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"left\"><b><font face='Courier New' color=\"black\" align=\"left\" size=4>Copyright.</font></b>\n",
        "<img alt=\"udeA logo\" height=\"120px\" src=\"https://github.com/freddyduitama/images/blob/master/in2lab.png?raw=true\" align=\"right\" hspace=\"10px\" vspace=\"0px\" height=\"120\" width=\"350\"\">\n",
        "                                                                                                                              \n",
        "<font face='Verdana' size=2>\n",
        "John Freddy Duitama M.<br>\n",
        "Universidad de Antioquia.<br>\n",
        "Apartado Aéreo 1226 | Dirección: calle 67 No. 53 - 108.<br>\n",
        "Medellín, Colombia. South America.\n",
        "    \n",
        "</p>\n",
        "</font>"
      ],
      "metadata": {
        "id": "qABbmViFmDXw"
      }
    }
  ]
}